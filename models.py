# -*- coding: utf-8 -*-
"""models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1464xVIPjP6AcN0G4_6ksOZHsGxsqOJCw

## **Dataset Setup & Preprocessing**

We'll walk through:

1. Loading both lab + real-world tomato datasets.

2. Preprocessing images (resize, normalize).

3. Splitting into train/val/test sets.

4. Preparing for transfer learning.

## **üß† Goal:**
Build ONE AI model that can detect tomato diseases accurately in real-world farm conditions by:

üìä Training on clean lab images (PlantVillage)

‚úÖ Testing on real-world field images (Field dataset)

## **‚úÖ 1. Google Colab Setup**
"""

!pip install tensorflow matplotlib scikit-learn

"""##**‚úÖ 2. Import Libraries**"""

import tensorflow as tf
import numpy as np
import os
import matplotlib.pyplot as plt
import zipfile
import shutil
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from google.colab import files

"""##**‚úÖ 3. Dataset Download (PlantVillage)**"""

from google.colab import files
files.upload()  # Upload the ZIP file here

import shutil
import os

# Rename the uploaded file to "kaggle.json"
shutil.move("kaggle (1).json", "kaggle.json")

# Create the .kaggle folder and move the API key there
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

print("‚úÖ Kaggle API configured successfully!")

!kaggle datasets download -d emmarex/plantdisease

!unzip -q plantdisease.zip -d /content/plantvillage_raw

"""##**list all folders (classes) inside the dataset like this:**

"""

import os

data_dir = '/content/plantvillage_raw/PlantVillage'
classes = os.listdir(data_dir)

# Print all class folder names
print(f"üìÅ Found {len(classes)} folders:\n")
for cls in sorted(classes):
    print("-", cls)

"""## **üîπ STEP 4: Filter Tomato Folders from PlantVillage**"""

import os
import shutil

# Source folder (original unzipped dataset)
source_dir = '/content/plantvillage_raw/PlantVillage'

# Destination folder for filtered tomato classes
filtered_dir = '/content/plantvillage_tomato'
os.makedirs(filtered_dir, exist_ok=True)

# List of tomato folders to keep
keep_classes = [
    'Tomato_Early_blight',
    'Tomato_Late_blight',
    'Tomato_healthy'
]

# Copy only selected tomato folders
for folder in keep_classes:
    src = os.path.join(source_dir, folder)
    dst = os.path.join(filtered_dir, folder)
    if os.path.exists(src):
        shutil.copytree(src, dst)

print("‚úÖ Filtered tomato dataset ready at:", filtered_dir)

"""##**üîπ STEP 5: Preprocess Images for Training & Validation**"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SIZE = 256
BATCH_SIZE = 32

# Image data generator with 80/20 train-validation split
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

# Load training data
train_generator = datagen.flow_from_directory(
    '/content/plantvillage_tomato',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

# Load validation data
val_generator = datagen.flow_from_directory(
    '/content/plantvillage_tomato',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=True
)

"""## **üîπ STEP 6: Build Model with MobileNetV3**"""

import tensorflow as tf

# Load MobileNetV3 as base
base_model = tf.keras.applications.MobileNetV3Large(
    input_shape=(IMG_SIZE, IMG_SIZE, 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False  # Freeze base layers

# Build full model
model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')
])

# Compile
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

"""## **üîπ STEP 7: Train the Model**"""

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=20
)

# Save the trained model in new format
model.save("tomato_model.keras")
print("‚úÖ Model saved successfully!")

"""## **üîπ STEP 8: Upload & Preprocess Field Dataset (Real-World Testing)**

**üìå Upload Field Dataset ZIP**
"""

from google.colab import files
files.upload()  # Upload the field dataset ZIP

import os

# Rename "field.zip.zip" to "field.zip"
os.rename("field.zip.zip", "field.zip")

"""**üìå Extract and Filter Tomato Classes**

"""

import zipfile, shutil

# Extract the renamed ZIP
with zipfile.ZipFile('field.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/field_data_full')

# Copy only tomato folders
source_field = '/content/field_data_full'
target_field = '/content/field_tomato'
os.makedirs(target_field, exist_ok=True)

field_keep = [
    'tomato___early_blight',
    'tomato___late_blight',
    'tomato___healthy'
]

for folder in os.listdir(source_field):
    folder_path = os.path.join(source_field, folder)
    if os.path.isdir(folder_path) and folder.lower() in field_keep:
        shutil.copytree(folder_path, os.path.join(target_field, folder))

print("‚úÖ Field dataset (tomato only) ready at /content/field_tomato")

!ls /content/field_data_full

import shutil, os

# ‚úÖ Use correct nested folder
source_field = '/content/field_data_full/field_tomato_dataset'
target_field = '/content/field_tomato'
os.makedirs(target_field, exist_ok=True)

# ‚úÖ Only copy these folders
field_keep = [
    'tomato___early_blight',
    'tomato___late_blight',
    'tomato___healthy'
]

for folder in os.listdir(source_field):
    folder_path = os.path.join(source_field, folder)
    if os.path.isdir(folder_path) and folder.lower() in field_keep:
        shutil.copytree(folder_path, os.path.join(target_field, folder))

print("‚úÖ Field dataset (tomato only) ready at /content/field_tomato")

!ls /content/field_tomato

"""## **üîπ STEP 9: Evaluate Model on Field Data (Real-World Generalization)**

**‚úÖ Then reload your dataset:**
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

field_datagen = ImageDataGenerator(rescale=1./255)

field_generator = field_datagen.flow_from_directory(
    '/content/field_tomato',
    target_size=(256, 256),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

"""**Evaluate Your Model on Real-World Data**"""

# Evaluate model on field data
loss, accuracy = model.evaluate(field_generator, verbose=1)
print(f"üß™ Real-World Accuracy: {accuracy * 100:.2f}%")

"""##**final stage now ‚Äî testing and visualization.**

##**‚úÖ Part 1: üìà Accuracy/Loss Graphs**
Run this to visualize your model‚Äôs training progress:
"""

import matplotlib.pyplot as plt

# Accuracy Plot
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Loss Plot
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""##**‚úÖ Part 2: üß™ Test a Single Image Manually (Field Data)**
First, make sure your field dataset is preprocessed like before.
"""

from tensorflow.keras.preprocessing import image
import numpy as np

# Upload your own image
from google.colab import files
uploaded = files.upload()

for fname in uploaded.keys():
    img_path = fname
    img = image.load_img(img_path, target_size=(256, 256))
    img_array = image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    prediction = model.predict(img_array)
    class_index = np.argmax(prediction)
    class_labels = list(train_generator.class_indices.keys())

    print(f"‚úÖ Prediction: {class_labels[class_index]}")

"""##**‚úÖ Part 3: üìä Visualize Predictions on Multiple Field Images**
Let‚Äôs see how the model performs on actual real-world field images:
"""

import matplotlib.pyplot as plt

# Load some field test images (adjust as needed)
field_generator.reset()
images, labels = next(field_generator)

preds = model.predict(images)
pred_classes = np.argmax(preds, axis=1)
true_classes = np.argmax(labels, axis=1)
class_labels = list(field_generator.class_indices.keys())

# Display first 5 predictions
plt.figure(figsize=(15, 5))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(images[i])
    plt.axis('off')
    plt.title(f"True: {class_labels[true_classes[i]]}\nPred: {class_labels[pred_classes[i]]}")
plt.tight_layout()
plt.show()

"""##**üëá ‚Äî these three steps help you analyze and prove how well your AI model actually performs in the real world.**

##**‚úÖ PART 1: Save Predictions as a CSV**
"""

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.image import img_to_array
import os

# Reset generator
field_generator.reset()

filenames = field_generator.filenames
preds = model.predict(field_generator, verbose=1)
pred_classes = np.argmax(preds, axis=1)
true_classes = field_generator.classes
class_labels = list(field_generator.class_indices.keys())
confidences = np.max(preds, axis=1)

# Build DataFrame
df = pd.DataFrame({
    "image_name": filenames,
    "predicted_label": [class_labels[i] for i in pred_classes],
    "true_label": [class_labels[i] for i in true_classes],
    "confidence": confidences
})

# Save CSV
df.to_csv("field_predictions.csv", index=False)
print("‚úÖ Predictions saved as field_predictions.csv")

"""##**‚úÖ PART 2: Plot Confidence Scores for First 10 Images**"""

import matplotlib.pyplot as plt
import numpy as np

# Ensure class_labels is defined properly
class_labels = list(train_generator.class_indices.keys())

# Loop through first 10 batches, show one image per batch
for i in range(10):
    img_batch, label_batch = field_generator[i]

    # Predict on image batch
    pred_probs = model.predict(img_batch)

    # Only use the FIRST image in the batch
    img = img_batch[0]
    pred = pred_probs[0]

    pred_class = np.argmax(pred)
    confidence = np.max(pred)

    plt.imshow(img)
    plt.axis('off')
    plt.title(f"Pred: {class_labels[pred_class]} ({confidence:.2%})")
    plt.show()

"""##**‚úÖ PART 3: Confusion Matrix (with Heatmap)**"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Generate confusion matrix
cm = confusion_matrix(true_classes, pred_classes)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("üìâ Confusion Matrix")
plt.show()

from google.colab import files
files.download("tomato_disease_model.h5")

from google.colab import files
files.download("/content/tomato_model.keras")